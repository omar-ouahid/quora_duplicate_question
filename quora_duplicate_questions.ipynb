{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omarinho1230/quora_duplicate_question/blob/master/quora_duplicate_questions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBhq3mfXLXwP",
        "colab_type": "text"
      },
      "source": [
        "### **import data from kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoAZoFQOWq2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kaggle\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kke2o2sBhEFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir .kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-6Oa-1kYoFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xwK0ccIXa0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "token = {\"username\":\"username\",\"key\":\"key\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:json.dump(token, file)\n",
        "!cp '/content/.kaggle/kaggle.json' ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWgp381IW4_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle datasets download -d quora/question-pairs-dataset\n",
        "!unzip \\*.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ts93mmBBL0n0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-ZEJyF4MkmI",
        "colab_type": "text"
      },
      "source": [
        "### **Dataset preparation and preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1TrNfKWkIsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('questions.csv') #dataframe///\n",
        "data = data.drop(['id', 'qid1', 'qid2'], axis=1)\n",
        "data.head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQt58s7eo2jf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data[:20000]\n",
        "data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lySiUjOTkItU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-YgvCtz6YDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[data.isna().any(axis=1)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_41x1b86sdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data=data.dropna()\n",
        "data = data.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIwvwKbakgdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_special(text):\n",
        "    special_chars = \"[~#$%^@&*&()+-_\\\",?!.:[]\\\\;><`|{}=\\/\\'=»¿シし]\"\n",
        "    for k in special_chars:\n",
        "            if type(text) == str:\n",
        "                if k==\"-\" or k==\"_\":\n",
        "                    text=text.replace(k, \"\")\n",
        "                else:\n",
        "                    text=text.replace(k, \" \")       \n",
        "    return text\n",
        "\n",
        "def contractions(sent):\n",
        "    sub_pattern = [(\"will not\",\"won't\"),(\"shall not\",\"shan't\"),\n",
        "                (\" not\", \"n\\'t\"),(\" will\",\"\\'ll\"),(\" is\",\"\\'s\"),\n",
        "                   (\" am\",\"\\'m\"),(\" are\",\"\\'re\"),(\" is\",\"who\\'s\")]\n",
        "    sent2=sent.split(\" \")\n",
        "    hold = \"\"\n",
        "    for k in range(len(sent2)):\n",
        "        kk = sent2[k]\n",
        "        for rep in range(len(sub_pattern)):\n",
        "            kk = re.sub(sub_pattern[rep][1],sub_pattern[rep][0],kk)\n",
        "        hold = hold + \" \" + kk\n",
        "    return hold.lower()\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    \n",
        "    stops1 = [word.lower() for word in stop_words]\n",
        "    punctuation = [',','.','!','?',';','-']\n",
        "    hold = []\n",
        "    if type(text) == list:\n",
        "        for word in range(len(text)):\n",
        "            if text[word].lower() in stops1 or text[word].lower() in punctuation or text[word].lower() == \"xxxxxx\":\n",
        "                continue\n",
        "            else:\n",
        "                hold.append(text[word].lower())\n",
        "    return hold\n",
        "\n",
        "def wordTok(sent):\n",
        "    tok = word_tokenize(sent)\n",
        "    return tok\n",
        "\n",
        "def pipeline(text):\n",
        "    text = contractions(text)\n",
        "    text = remove_special(text)\n",
        "    textToks = wordTok(text)\n",
        "    textToks = remove_stop_words(textToks)\n",
        "    final = \"\"\n",
        "    for k in range(len(textToks)):\n",
        "            final = final+textToks[k]+\" \"\n",
        "    return final.strip().lower()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpLy1qm0CTgG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " import nltk\n",
        " nltk.download('punkt')\n",
        "data[\"question1\"] = list(map(pipeline,data[\"question1\"]))\n",
        "data[\"question2\"] = list(map(pipeline,data[\"question2\"]))\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F65ZorxqikW",
        "colab_type": "text"
      },
      "source": [
        "### **length based features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NElfsITQkItv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "data['len_q1'] = data.question1.apply(lambda x: len(str(x)))\n",
        "data['len_q1'].head()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Skluf9m_kIuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['len_q2'] = data.question2.apply(lambda x: len(str(x)))\n",
        "data['len_q2'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HfOlBeCSE8e",
        "colab_type": "text"
      },
      "source": [
        "### **difference in lengths of two questions**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lhh7N6E-kIuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "data['diff_len'] = data.len_q1 - data.len_q2\n",
        "data['diff_len'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIIgAFPpSfSG",
        "colab_type": "text"
      },
      "source": [
        "### **character length based features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAUyWeGpkIuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "data['len_char_q1'] = data.question1.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "data['len_char_q1'].head()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01kKLm5ukIuv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['len_char_q2'] = data.question2.apply(lambda x: len(''.join(set(str(x).replace(' ', '')))))\n",
        "data['len_char_q2'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAgDZFKLrH9r",
        "colab_type": "text"
      },
      "source": [
        "### **word length based features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYoB4OjlkIvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data['len_word_q1'] = data.question1.apply(lambda x: len(str(x).split()))\n",
        "data['len_word_q1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y06K_JjQkIvY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['len_word_q2'] = data.question2.apply(lambda x: len(str(x).split()))\n",
        "data['len_word_q2']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RddrTdO7kIvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# common words in the two questions\n",
        "data['common_words'] = data.apply(lambda x: len(set(str(x['question1']).lower().split())\n",
        ".intersection(set(str(x['question2']).lower().split()))), axis=1)\n",
        "data['common_words'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QoKGlNAkIwK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fs_1 = ['len_q1', 'len_q2', 'diff_len', 'len_char_q1', \n",
        "        'len_char_q2', 'len_word_q1', 'len_word_q2',     \n",
        "        'common_words']   #list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUU8yQa1QjBf",
        "colab_type": "text"
      },
      "source": [
        "number of words we have in this dataset is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0ab4omRvmnV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import word_tokenize\n",
        "allWords = \"\"\n",
        "for k in range (len(data[\"question1\"])):\n",
        "    allWords=allWords+data[\"question1\"][k].strip().lower()+\" \"\n",
        "    allWords=allWords+data[\"question2\"][k].strip().lower()+\" \"\n",
        "allcount = len(set(word_tokenize(allWords)))\n",
        "print(allcount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQJQGATEQzWg",
        "colab_type": "text"
      },
      "source": [
        "**features of similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm3uzc7akIwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install fuzzywuzzy[speedup]\n",
        "from fuzzywuzzy import fuzz\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1afw62XmTKZY",
        "colab_type": "text"
      },
      "source": [
        "**compares the entire question similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pKsTcR-kIwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['fuzz_qratio'] = data.apply(lambda x: fuzz.QRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_qratio'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKI85gaDTjpL",
        "colab_type": "text"
      },
      "source": [
        "**compares partial question similarity**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxSqlKL8kIw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['fuzz_partial_ratio'] = data.apply(lambda x: fuzz.partial_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_partial_ratio'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHp6oSiGVOgF",
        "colab_type": "text"
      },
      "source": [
        "**Wratio to eliminate lower and upper cases and more**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfZooa-akIwz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['fuzz_WRatio'] = data.apply(lambda x: fuzz.WRatio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_WRatio'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHz9p73mV5OR",
        "colab_type": "text"
      },
      "source": [
        "**partial token set ratio**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuRmGdTQkIxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data['fuzz_partial_token_set_ratio'] = data.apply(lambda x: fuzz.partial_token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "\n",
        "data['fuzz_partial_token_set_ratio'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWXRoTE8kIxQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['fuzz_partial_token_sort_ratio'] = data.apply(lambda x: fuzz.partial_token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_partial_token_sort_ratio'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0rANiWQkIxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data['fuzz_token_set_ratio'] = data.apply(lambda x: fuzz.token_set_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_token_set_ratio'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX8kR6hpkIxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "data['fuzz_token_sort_ratio'] = data.apply(lambda x: fuzz.token_sort_ratio(str(x['question1']), str(x['question2'])), axis=1)\n",
        "data['fuzz_token_sort_ratio'].head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz6MvUDnkIx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "fs_2 = ['fuzz_qratio', 'fuzz_WRatio', 'fuzz_partial_ratio', \n",
        "       'fuzz_partial_token_set_ratio', 'fuzz_partial_token_sort_ratio',\n",
        "       'fuzz_token_set_ratio', 'fuzz_token_sort_ratio']  #list\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFf4gCtoXI8d",
        "colab_type": "text"
      },
      "source": [
        "**import word2vec pre-trained Google News corpus word vector model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idjj7p1Vx40g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!brew install wget\n",
        "\n",
        "!wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9mzAndTYQne",
        "colab_type": "text"
      },
      "source": [
        "**load the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk-1SNS9kIx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim import models\n",
        "EMBEDDING_FILE = 'GoogleNews-vectors-negative300.bin.gz'\n",
        "model =models.KeyedVectors.load_word2vec_format(\n",
        "    EMBEDDING_FILE, binary=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5ukX4gJkIyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr4fL2YFm6XW",
        "colab_type": "text"
      },
      "source": [
        "**Word mover distance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy61IF76geXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wmd(s1, s2):\n",
        "    s1 = str(s1).lower().split()\n",
        "    s2 = str(s2).lower().split()\n",
        "    stop_words = stopwords.words('english')\n",
        "    s1 = [w for w in s1 if w not in stop_words]\n",
        "    s2 = [w for w in s2 if w not in stop_words]\n",
        "    return model.wmdistance(s1, s2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT6WJPVoglsA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['wmd'] = data.apply(lambda x: wmd(data['question1'], data['question2']), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3uC4ckBnYra",
        "colab_type": "text"
      },
      "source": [
        "**transform words to vectors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1x0vW8bkIyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent2vec(s, model): \n",
        "    M = []\n",
        "    words = str(s).lower().split()\n",
        "    for word in words:\n",
        "      if word not in stop_words:\n",
        "        if word.isalpha():\n",
        "          if word in model:\n",
        "            M.append(model[word])\n",
        "    M = np.array(M)\n",
        "    if len(M) > 0:\n",
        "        v = M.sum(axis=0)\n",
        "        return v / np.sqrt((v ** 2).sum())\n",
        "    else:\n",
        "        return np.zeros(300)  #1*300\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S4e4byHkIyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_q1 = np.array([sent2vec(q, model) for q in data.question1])\n",
        "w2v_q2 = np.array([sent2vec(q, model) for q in data.question2])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3-xFxH1nqCb",
        "colab_type": "text"
      },
      "source": [
        "**padding the vectors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b6-DbGB6Y1E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "STRING_SIZE= max(data.question1.map(lambda x: len(x)).max(),\n",
        "                     data.question2.map(lambda x: len(x)).max())\n",
        "sent1pad = pad_sequences(w2v_q1, maxlen=STRING_SIZE,padding='post',truncating='post',dtype='float32')\n",
        "sent2pad = pad_sequences(w2v_q2,maxlen=STRING_SIZE,padding='post',truncating='post',dtype='float32')\n",
        "sent1pad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH5h0BEHoEzQ",
        "colab_type": "text"
      },
      "source": [
        "**cosine distance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUxjVXfYkIyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "data['cosine_distance'] = [cosine(x,y) for (x,y) in zip(w2v_q1, w2v_q2)]  #x y 1-D array\n",
        "\n",
        "fs4_1 = ['cosine_distance','wmd']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EHPYb1prwYA",
        "colab_type": "text"
      },
      "source": [
        "**scale the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIJiTnl_kIyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzwzvF0EkIy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()\n",
        "y = data.is_duplicate.values  #ndarray\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToQ2DYQCkIy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = y.astype('float32').reshape(-1, 1)  #ndarray\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnmPvFnH3hAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y=pd.DataFrame(data=y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcNOe4c9ixm_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data.copy().drop([\"question1\", \"question2\",\"is_duplicate\"], axis=1)\n",
        "X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU1WJBzPkIzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X.replace([np.inf, -np.inf], np.nan).fillna(0).values  #ndarray\n",
        "X = scaler.fit_transform(X)  #ndarray\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZzaPiYmsAF-",
        "colab_type": "text"
      },
      "source": [
        "**split the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN3J84_ukIzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(0)  #same results will be generated although shuffle is used\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1lV5vQ4sRbf",
        "colab_type": "text"
      },
      "source": [
        "**logistic regression model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlGAxTOtkIza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logres = linear_model.LogisticRegression(C=0.1, \n",
        "                                 solver='sag', max_iter=5000)\n",
        "                               \n",
        "logres.fit(x_train, y_train.ravel())\n",
        "\n",
        "lr_preds = logres.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0W_tG77kIzn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgZ2gVqmkIzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "cm = confusion_matrix(y_test, lr_preds)\n",
        "\n",
        "cm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK3l_1rnp2s0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"logistic regression accuracy: \"+str(accuracy_score(lr_preds, y_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoKYLOh3weON",
        "colab_type": "text"
      },
      "source": [
        "**some other models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmBKvPnYkIzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SESpLrbLkIzy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LinearDiscriminantAnalysis_model=LinearDiscriminantAnalysis()\n",
        "KNeighborsClassifier_model=KNeighborsClassifier()\n",
        "GaussianNB_model=GaussianNB()\n",
        "DecisionTreeClassifier_model=DecisionTreeClassifier()\n",
        "SVC_model=SVC()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1vhafnIkIz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "KNeighborsClassifier_model.fit(x_train, y_train.ravel())\n",
        "GaussianNB_model.fit(x_train, y_train.ravel())\n",
        "DecisionTreeClassifier_model.fit(x_train, y_train.ravel())\n",
        "SVC_model.fit(x_train, y_train.ravel())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayvSrakakI0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_preds = GaussianNB_model.predict(x_test)\n",
        "knn_preds = KNeighborsClassifier_model.predict(x_test)\n",
        "dtc_preds = DecisionTreeClassifier_model.predict(x_test)\n",
        "svc_preds = SVC_model.predict(x_test)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63pZrMumkI0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accuracy score is the simplest way to evaluate\n",
        "print(\"naive bayesian accuracy:\"+str(accuracy_score(nb_preds, y_test)))\n",
        "print(\"knn accuracy:\"+str(accuracy_score(knn_preds, y_test)))\n",
        "print(\"decision tree accuracy:\"+str(accuracy_score(dtc_preds, y_test)))\n",
        "print(\"support vector machines accuracy :\"+str(accuracy_score(svc_preds, y_test)))\n",
        "# But Confusion Matrix and Classification Report give more details about performance\n",
        "print(\"===================knn===========================================\")\n",
        "print(classification_report(knn_preds, y_test))\n",
        "print(\"===================naive bayesian================================\")\n",
        "print(classification_report(nb_preds, y_test))\n",
        "print(\"===================decision tree=================================\")\n",
        "print(classification_report(dtc_preds, y_test))\n",
        "print(\"===================support vector machines=======================\")\n",
        "print(classification_report(svc_preds, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9hY_RDSwusC",
        "colab_type": "text"
      },
      "source": [
        "**artificial neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zjzOo_xJqT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhM4iqWtCiC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7EuJ0ZoFEpY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = Sequential()\n",
        "#First Hidden Layer\n",
        "classifier.add(Dense(17, activation='relu', kernel_initializer='random_normal', input_dim=17))\n",
        "#Second  Hidden Layer\n",
        "classifier.add(Dense(100, activation='tanh', kernel_initializer='random_normal'))\n",
        "#Third  Hidden Layer\n",
        "classifier.add(Dense(100, activation='tanh', kernel_initializer='random_normal'))\n",
        "#Fourth  Hidden Layer\n",
        "classifier.add(Dense(100, activation='relu', kernel_initializer='random_normal'))\n",
        "#Fifth  Hidden Layer\n",
        "classifier.add(Dense(50, activation='relu', kernel_initializer='random_normal'))\n",
        "#Output Layer\n",
        "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDC3UltGFWH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compiling the neural network\n",
        "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJLEn4bRORVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(classifier, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmdKXdoMJJJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fitting the data to the training dataset\n",
        "modelann=classifier.fit(x_train, y_train.ravel(),verbose=True, batch_size=10, epochs=50 ,validation_data=(x_test, y_test.ravel()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIig5oV4JXBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eval_model=classifier.evaluate(x_test, y_test.ravel())\n",
        "eval_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36wWjvEVM7FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred=classifier.predict(x_test)\n",
        "y_pred =(y_pred>0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKPPWKP8NICV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOflF1ZCxGIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkV2RM96NONq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(modelann.history['acc'])\n",
        "plt.plot(modelann.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(modelann.history['loss'])\n",
        "plt.plot(modelann.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezgCZnWFyiKO",
        "colab_type": "text"
      },
      "source": [
        "**prepare the data to siamese lstm model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H4w6TH1o73l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "split=int(0.8*len(sent1pad))\n",
        "\n",
        "y = data[\"is_duplicate\"]\n",
        "x1_train = sent1pad[:split]\n",
        "x1_val = sent2pad[split:]\n",
        "x2_train = sent2pad[:split]\n",
        "x2_val = sent1pad[split:]\n",
        "y_train = y[:split]\n",
        "y_val = y[split:]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x1_train = np.asarray(x1_train,dtype='float32')\n",
        "x1_val = np.asarray(x1_val,dtype='float32')\n",
        "x2_train = np.asarray(x2_train,dtype='float32')\n",
        "x2_val = np.asarray(x2_val,dtype='float32')\n",
        "y_train = np.asarray(y_train,dtype='float32')\n",
        "y_val = np.asarray(y_val,dtype='float32')\n",
        "\n",
        "TRAIN_SIZE = len(x1_train)\n",
        "VAL_SIZE = len(x1_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlOAklopXFWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x1_train.shape)\n",
        "print(x1_val.shape)\n",
        "print(x2_train.shape)\n",
        "print(x2_val.shape)\n",
        "print(y_train.shape)\n",
        "print(y_val.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiEE7TgYqeR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "EMBEDDING_SIZE=256\n",
        "x1_train\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Hfd5frtZ0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33mK9z4qBssZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y4RWHUz98Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjcVOACa1Vy6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def exponent_neg_manhattan_distance(left, right):\n",
        "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
        "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLlLq8DnD8qD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x2_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCA1ksKGwg1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "import datetime\n",
        "from keras.layers import Dense,LSTM,Input,Activation,Flatten,Concatenate,Reshape,Embedding,Bidirectional,Dropout, Lambda,BatchNormalization\n",
        "from keras.models import Model,Sequential\n",
        "import keras.backend as K\n",
        "from keras.optimizers import Adadelta\n",
        "gradient_clipping_norm = 1.25\n",
        "model1_input = Input(shape=(x1_train.shape[1],),name=\"input1\")\n",
        "model1_embedding = Embedding(allcount,output_dim=EMBEDDING_SIZE,input_length=STRING_SIZE,name=\"embedding1\")(model1_input)\n",
        "model1_LSTM = LSTM(256,return_sequences=False,activation='relu',dropout=0.4,name=\"LSTM1\")(model1_embedding)\n",
        "\n",
        "\n",
        "model2_input = Input(shape=(x2_train.shape[1],),name='input2')\n",
        "model2_embedding = Embedding(allcount,output_dim=EMBEDDING_SIZE,input_length=STRING_SIZE,name=\"embedding2\")(model2_input)\n",
        "model2_LSTM = LSTM(256,return_sequences=False,activation='relu',dropout=0.4,name=\"LSTM2\")(model2_embedding)\n",
        "\n",
        "\n",
        "# Calculates the distance as defined by the MaLSTM model\n",
        "malstm_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([model1_LSTM, model2_LSTM])\n",
        "normalization=BatchNormalization()(malstm_distance)\n",
        "before_last_layer=Dense(1, activation='tanh', kernel_initializer='glorot_uniform',name=\"before_output\")(normalization)\n",
        "normalization2=BatchNormalization()(before_last_layer)\n",
        "output=Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform',name=\"output\")(normalization2)\n",
        " #Pack it all up into a model\n",
        "malstm = Model([model1_input, model2_input], [output])\n",
        "\n",
        "# Adadelta optimizer, with gradient clipping by norm\n",
        "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
        "\n",
        "malstm.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Start training\n",
        "training_start_time = time()\n",
        "\n",
        "malstm_trained = malstm.fit([x1_train, x2_train], y_train, batch_size=100, nb_epoch=10,\n",
        "                            validation_data=([x1_val, x2_val], y_val))\n",
        "\n",
        "print(\"Training time finished.\\n{} epochs in {}\".format(30, datetime.timedelta(seconds=time()-training_start_time)))\n",
        "malstm.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSSFF5AIqrxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "malstm.evaluate([x1_val,x2_val],y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k28S50YCwYPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(malstm, to_file='model.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzxF4MCyem0u",
        "colab_type": "text"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfoyI3RY_ylI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(malstm_trained.history['accuracy'])\n",
        "plt.plot(malstm_trained.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(malstm_trained.history['loss'])\n",
        "plt.plot(malstm_trained.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAI83h8WxnkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}